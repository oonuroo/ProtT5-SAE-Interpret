# Interpreting Disordered Proteins Using Sparse Autoencoders and Protein Language Models ğŸ§¬

This repository contains code and documentation for interpreting **disordered proteins** using **Sparse Autoencoders (SAEs)** on embeddings from the **ProtT5** protein language model (pLM). Inspired by *From Mechanistic Interpretability to Mechanistic Biology* ([bioRxiv](https://www.biorxiv.org/content/10.1101/2024.08.27.609883v1)), it modifies and adapts the architecture from [etowahadams/interprot](https://github.com/etowahadams/interprot). 

## Project Aim ğŸ¯
The goal is to:
- Develop an SAE for ProtT5 to capture features of disordered proteins beyond sequence-derived properties.
- Construct datasets for protein properties (e.g., disorder regions, phase separation) from MobiDB, UniProt, and PDB.
- Probe SAE neurons to identify sequence positions linked to these properties.
- Visualize feature-associated regions to understand ProtT5's encoding.
- Integrate the pipeline into the [FELLS web server](https://fells.org/).

## Dataset ğŸ“Š
- **Source**: **280,589 protein sequences** in FASTA format with diverse cellular contexts.
- **Preprocessing**:
  - Verified uniqueness of protein IDs.
  - Performed exploratory data analysis (EDA):
    - Sequence length stats: min, max, average, median.
    - Amino acid length distribution.
  - Prepared for ProtT5:
    - Added whitespaces between amino acids.
    - Replaced rare amino acids (U, Z, O, B) with 'X'.
- **Notes**:
    -  Embedding file sizes are  (707 GB)
    -  One sequence is processed at a time
## Methodology âš™ï¸

### 1. ProtT5 Embeddings Extraction
- **Model**: `ProtT5-XL-Half-UniRef50-Enc` ([Hugging Face](https://huggingface.co/Rostlab/prot_t5_xl_half_uniref50-enc)).
- **Grid Search**:
  - Tested 1,000 random proteins to optimize extraction.
  - Parameters:
    - Queue sizes: `[50, 100, 500]`
    - Batch sizes: `[25, 50, 100, 250]`
    - Compression: `[gzip, lzf]`
    - Threading: `[True, False]`
  - Total: **48** configurations (3 Ã— 4 Ã— 2 Ã— 2).
  - Best setup chosen based on runtime.
- **Extraction**:
  - Per-residue embeddings from:
    - **Layer 16** (middle): `layer_16.h5` (470 GB).
    - **Layer 24** (output): `layer_24.h5` (235 GB).
  - Total: **707 GB**.
  - Logged sequence IDs to ensure data integrity (no duplications or missing proteins).

### 2. Sparse Autoencoder (SAE) Implementation
- **Architecture**: Adapted from [etowahadams/interprot](https://github.com/etowahadams/interprot) for ProtT5 embeddings.
- **Grid Search**:
  - Trained on 2,000 sequences.
  - Parameters:
    - Sparsity (k): `[16, 32, 64, 128, 256]`
    - Hidden dimensions: `[2048, 4096, 8192, 16384]`
    - Layers: `[16, 24]`
    - Learning rates: `[1e-4, 2e-4, 5e-4]`
  - Total: **120** configurations (5 Ã— 4 Ã— 2 Ã— 3).
  - Evaluated via reconstruction loss and sparsity, tracked with [Weights & Biases (WandB)](https://wandb.ai/) and CSV logs.
- **Training**: SAE learns sparse representations for feature extraction.

### 3. Dataset Construction (In Progress) ğŸ“‹
- **Sources**:
  - **MobiDB**: Disorder regions, phase separation, compactness ([TSV](https://mobidb.org/api/download?acc=P04637&format=tsv), [JSON](https://mobidb.org/)).
  - **UniProt**: Signal peptides, transmembrane regions ([API](https://rest.uniprot.org/)).
  - **PDB/MobiDB**: Secondary structure (planned, via MobiDB MongoDB).
  - **Other**: Aggregation (PASTA3), phosphorylation (Scop3P).
- **Properties**:
  - Disorder regions, phase separation, fold-upon-binding (â‰¥10 residues), subcellular localization, signal peptides, transmembrane regions, secondary structure, aggregation, phosphorylation.
- **Redundancy**: Targeting â‰¤25% sequence identity.

### 4. Probing and Visualization (Planned) ğŸ“ˆ
- Probe SAE neurons for property-specific signals.
- Map active neurons to sequence positions.
- Integrate into FELLS for interactive visualization.

## Repository Structure

```plaintext
â”œâ”€â”€ src/
   â”œâ”€â”€ data-preparation/
       â”œâ”€â”€ clean_sequences.py                   # Clean dataset (add spaces, replace rare amino acids with 'X')
       â”œâ”€â”€ cleaned_gridsearch.py                # Grid search for cleaned sequences
       â”œâ”€â”€ data_discovery.ipynb                 # EDA for embeddings and dataset
   â”œâ”€â”€ model-save/
       â”œâ”€â”€ get_model.py                         # Download and save ProtT5 model (~5 GB)
   â”œâ”€â”€ sae/
       â”œâ”€â”€ prot_t5_wrapper.py                   # Extract ProtT5 embeddings
       â”œâ”€â”€ data_module.py                       # Prepare train, validation, test datasets
       â”œâ”€â”€ sae_model.py                         # Initialize SAE
       â”œâ”€â”€ sae_module.py                        # SAE model setup, training, validation, early stopping
       â”œâ”€â”€ test_hyper_grid_search.py            # Test grid search with smaller dataset
       â”œâ”€â”€ hyper_grid_search.py                 # Full grid search
       â”œâ”€â”€ grid_search_summarize_results.py     # Save grid search results to CSV
       â”œâ”€â”€ extract_sae_activations.py           # Extract SAE neuron activations
       â”œâ”€â”€ extract_top_latents.py               # Identify top latent features
       â”œâ”€â”€ training.py                          # Train SAE and save results
   â”œâ”€â”€ utils/
       â”œâ”€â”€ utils.py                             # Helper functions (e.g., train-val-test split)
â””â”€â”€ README.md                                    # Project overview
```
